{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NC_BERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ge4Iy301rxOE"},"source":["!pip install transformers==3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQ9DO8G_oaAu"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZL1qDB7Hv61W"},"source":["from os import path\n","import pandas\n","import pandas as pd\n","pd.set_option('display.max_columns', None)\n","\n","import transformers\n","import itertools\n","import torch\n","from collections import defaultdict\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","from torch.utils import data\n","import json\n","import os\n","import copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfJIfckDN97-"},"source":["# configurations\n","BERT_MODEL_NAME = 'bert-base-uncased'\n","DUAL_SEQUENCE = False\n","BATCH_SIZE = 32\n","EPOCHS = 75\n","ITERATIONS = 10\n","SPLIT = 'random'\n","GRAIN = 'coarse'\n","REMOVE_NON_BINARY = False\n","REMOVE_NON_COMP = False\n","IMAGE_MODEL = 'ResNet152V2_10_pca_384_norm'\n","MULTIMODAL = True\n","FREEZE_BERT = True\n","JOIN_VAL_AND_TRAIN = False\n","\n","VEC_COMBO_MODE = 'concatenate'\n","MULTI_COMBO_MODE = 'concatenate'\n","\n","#This parameter toggles filtering of the data by the image model, \n","# even in unimodal mode, to ensure comparable results to the multimodal experiments.\n","UNIMODAL_COMPARABLE = True\n","\n","if UNIMODAL_COMPARABLE: \n","  SAVE_PREDS_TO = f'results_filtered_on_{IMAGE_MODEL}'\n","else: \n","  SAVE_PREDS_TO = 'results'\n","\n","## False, 'generic', or 'natural'\n","EMB_COMP = False\n","\n","if EMB_COMP:\n","  assert(not DUAL_SEQUENCE), 'For now, the use of dual_sequence with embedded compounds is not supported'\n","\n","tokenizer = transformers.BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjfwBs-usWvX"},"source":["def prep_data():\n","  directory = '/content/drive/My Drive/Tratz_2011_data_comp_binary'\n","  data_file = f'tratz2011_cb_{GRAIN}_grained_{SPLIT}'\n","  data_path = path.join(directory, data_file)\n","    \n","  train_data = pd.read_csv(path.join(data_path, 'train.tsv'), sep='\\t', header=None, index_col=None)\n","  test_data = pd.read_csv(path.join(data_path, 'test.tsv'), sep='\\t', header=None, index_col=None)\n","  val_data = pd.read_csv(path.join(data_path, 'val.tsv'), sep='\\t', header=None, index_col=None)\n","\n","  train_data.columns = ['nc_mod', 'nc_head', 'nc_type']\n","  test_data.columns = ['nc_mod', 'nc_head', 'nc_type']\n","  val_data.columns = ['nc_mod', 'nc_head', 'nc_type']\n","\n","  if not EMB_COMP:\n","\n","    train_data['compound'] = train_data['nc_mod'] + \" \" + train_data['nc_head']\n","    test_data['compound'] = test_data['nc_mod'] + \" \" + test_data['nc_head']\n","    val_data['compound'] = val_data['nc_mod'] + \" \" + val_data['nc_head']\n","  elif EMB_COMP == 'generic':\n","\n","    generic_sentence = 'we know for a fact that the phrase {} exists because we have seen it in use several times before'\n","    train_data['compound'] = [generic_sentence.format(m + ' ' + h) for m, h in zip(train_data['nc_mod'], train_data['nc_head'])] \n","    test_data['compound'] = [generic_sentence.format(m + ' ' + h) for m, h in zip(test_data['nc_mod'], test_data['nc_head'])] \n","    val_data['compound'] = [generic_sentence.format(m + ' ' + h) for m, h in zip(val_data['nc_mod'], val_data['nc_head'])] \n","  elif EMB_COMP == 'natural':\n","    with open('/content/drive/My Drive/compound_sents_filtered_May-25-2021.json', 'r') as sentence_file:\n","      sentence_file.seek(0)\n","      sentence_dict = json.load(sentence_file)\n","    generic_sentence = 'we know for a fact that the phrase {} exists because we have seen it in use several times before'\n","    compound_sentences_train = [sentence_dict[m + ' ' + h][0] if m + ' ' + h in sentence_dict else generic_sentence.format(m + ' ' + h) for m, h in zip(train_data['nc_mod'], train_data['nc_head'])]\n","    print(len(train_data), len(compound_sentences_train))\n","    assert(len(compound_sentences_train) == len(train_data)), 'there are still some compounds in the training data that dont have sentences'\n","    train_data['compound'] = compound_sentences_train\n","\n","    compound_sentences_test = [sentence_dict[m + ' ' + h] if m + ' ' + h in sentence_dict else generic_sentence.format(m + ' ' + h) for m, h in zip(test_data['nc_mod'], test_data['nc_head'])]\n","    assert(len(compound_sentences_test) == len(test_data)), 'there are still some compounds in the training data that dont have sentences'\n","    test_data['compound'] = compound_sentences_test\n","\n","    compound_sentences_val = [sentence_dict[m + ' ' + h] if m + ' ' + h in sentence_dict else generic_sentence.format(m + ' ' + h) for m, h in zip(val_data['nc_mod'], val_data['nc_head'])]\n","    assert(len(compound_sentences_val) == len(val_data)), 'there are still some compounds in the training data that dont have sentences'\n","    val_data['compound'] = compound_sentences_val\n","  \n","  labels_to_ids = {label: index for index, label in enumerate(train_data.nc_type.unique())}\n","\n","  ordered_labels = ['none']*len(labels_to_ids)\n","  for v, k in labels_to_ids.items():\n","    ordered_labels[k] = v\n","\n","\n","  train_data['label_id'] = train_data['nc_type'].replace(labels_to_ids)\n","  test_data['label_id'] = test_data['nc_type'].replace(labels_to_ids)\n","  val_data['label_id'] = val_data['nc_type'].replace(labels_to_ids)\n","  \n","  return train_data, test_data, val_data, labels_to_ids, ordered_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCrKYRnF9RoI"},"source":["def get_image_model(model_name):\n","  with open(f'/content/drive/My Drive/word_to_img_vec_{model_name}.json', 'r') as sentence_file:\n","      model = json.load(sentence_file)\n","  return model\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CN5laJ8Hq5gp"},"source":["def prep_image_data(data: pd.DataFrame, model):\n","  if VEC_COMBO_MODE == 'add':\n","    f = lambda x: x.split(' ')\n","    vectors = list(data['compound'].map(lambda x: np.add(model[f(x)[0]], model[f(x)[1]])))\n","  elif VEC_COMBO_MODE == 'average':\n","    f = lambda x: x.split(' ')\n","    vectors = list(data['compound'].map(lambda x: np.average([model[f(x)[0]], model[f(x)[1]]], axis=0)))\n","  elif VEC_COMBO_MODE == 'concatenate':\n","    f = lambda x: x.split(' ')\n","    vectors = list(data['compound'].map(lambda x: np.concatenate((model[f(x)[0]], model[f(x)[1]]))))\n","  else: \n","    raise ValueError('VEC_COMBO_MODE must be either `add` or `average`.')\n","  return vectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62jTwnkgLeHd"},"source":["# Removes words from the dataframe whose mod, head, or combined compound don't exist in a given model\n","def remove_non_existing(data: pd.DataFrame, model, filter_on: list):\n","    possible_to_filter_on = ['mod', 'head', 'compound']\n","    assert len(filter_on) > 0, 'Parameter filter_on must be a non-empty list'\n","    for word in filter_on:\n","        assert word in possible_to_filter_on, \\\n","            f'Parameter filter_on must be a list containing at least one of the following: {possible_to_filter_on}'\n","    def compound_exists_in_model(compound):\n","        one_word_compound = ''.join(compound.split())\n","        underscore_compound = '_'.join(compound.split())\n","        hyphenated_compound = '-'.join(compound.split())\n","        two_word_compound = ' '.join(compound.split())\n","\n","        if compound in model \\\n","                or underscore_compound in model \\\n","                or one_word_compound in model \\\n","                or hyphenated_compound in model:\n","            return True\n","        else: return False\n","\n","    def word_exists_in_model(word):\n","        if word in model:\n","            return True\n","        else:\n","            return False\n","\n","    new_data = copy.deepcopy(data)\n","    if 'compound' in filter_on:\n","        new_data = new_data[new_data['compound'].map(compound_exists_in_model)]\n","    if 'mod' in filter_on:\n","        new_data = new_data[new_data['nc_mod'].map(word_exists_in_model)]\n","    if 'head' in filter_on:\n","        new_data = new_data[new_data['nc_head'].map(word_exists_in_model)]\n","    return new_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHV1NuyJFxTq"},"source":["class TratzDataset(data.Dataset):\n","  def __init__(self, nc_data, labels, tokenizer, max_len):\n","    if isinstance(nc_data, pandas.core.series.Series):\n","      self.compounds = nc_data.to_numpy()\n","      assert len(labels) == len(self.compounds), \"Length of data list and labels list must be equal\"\n","      self.dualsequence = False\n","    elif len(nc_data) == 2:\n","      self.mods = nc_data[0].to_numpy()\n","      self.heads = nc_data[1].to_numpy()\n","      assert len(self.mods) == len(self.heads), \"Length of modifier list and head list must be equal\"\n","      assert len(labels) == len(self.heads), \"Length of data list and labels list must be equal\"\n","      self.dualsequence = True\n","    else: raise ValueError(\"nc_data must either be a Pandas series (df column) or a tuple of two such Pandas series\")\n","\n","    self.labels = labels\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    if self.dualsequence == True: \n","      return len(self.mods)\n","    else: return len(self.compounds)\n","\n","  def __getitem__(self, i):\n","    if self.dualsequence == True:\n","      mod, head = str(self.mods[i]), str(self.heads[i])\n","      encoding = tokenizer.encode_plus(mod, head,\n","                                    max_length = self.max_len, \n","                                    padding = 'max_length',\n","                                    return_tensors = 'pt',\n","                                    return_attention_mask = True,\n","                                    add_special_tokens = True,\n","                                    return_token_type_ids = True, \n","                                    truncation = False\n","                                    )\n","      return {\n","          'compound' : str(mod) + ' ' + str(head),\n","          'input_ids' : encoding['input_ids'].flatten(),\n","          'attention_mask' : encoding['attention_mask'].flatten(),\n","          'token_type_ids' : encoding['token_type_ids'].flatten(),\n","          'labels' : torch.tensor(self.labels[i], dtype=torch.long)\n","      }\n","    else: \n","      compound = str(self.compounds[i])\n","      encoding = tokenizer.encode_plus(compound,\n","                                    max_length = self.max_len, \n","                                    padding = 'max_length',\n","                                    return_tensors = 'pt',\n","                                    return_attention_mask = True,\n","                                    add_special_tokens = True,\n","                                    return_token_type_ids = True, \n","                                    truncation = False\n","                                    )\n","      return {\n","        'compound' : str(compound),\n","        'input_ids' : encoding['input_ids'].flatten(),\n","        'attention_mask' : encoding['attention_mask'].flatten(),\n","        'token_type_ids' : encoding['token_type_ids'].flatten(),\n","        'labels' : torch.tensor(self.labels[i], dtype=torch.long)\n","      }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hn1aDZ4L8LQx"},"source":["class ImageVectorsDataset(data.Dataset):\n","  def __init__(self, img_vectors):\n","    self.img_vectors = img_vectors\n","\n","  def __len__(self):\n","    return len(self.img_vectors)\n","\n","  def __getitem__(self, i):\n","    return {'img_vector' : self.img_vectors[i]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9ghT-xhSKdv"},"source":["def make_text_data_loader(df: pd.DataFrame, col_names: list, tokenizer: transformers.BertTokenizer, max_len: int, batch_size: int):\n","  if len(col_names) == 1:\n","    compound_col = col_names[0]\n","    dataset = TratzDataset(\n","      nc_data=df[compound_col],\n","      labels=df.label_id.to_numpy(), \n","      tokenizer=tokenizer,\n","      max_len=max_len\n","      )\n","  elif len(col_names) == 2:\n","    mod_col, head_col = col_names[0], col_names[1]\n","    dataset = TratzDataset(\n","      nc_data=(df[mod_col], df[head_col]),\n","      labels=df.label_id.to_numpy(), \n","      tokenizer=tokenizer,\n","      max_len=max_len\n","      )   \n","  else: raise ValueError(\"You must pass either one or two column names!\")\n","\n","  return data.DataLoader(\n","      dataset,\n","      batch_size=batch_size,\n","      num_workers=2\n","  )  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHVt5s-L5yRm"},"source":["def make_image_data_loader(vectors, batch_size):\n","  dataset = ImageVectorsDataset(vectors)\n","  return data.DataLoader(\n","      dataset, \n","      batch_size=batch_size,\n","      num_workers=2\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRTkGrY1cFdK"},"source":["def prep_data_loaders(train_data, test_data, val_data):\n","  # Making all the data loaders\n","  assert not (DUAL_SEQUENCE and EMB_COMP), 'both DUAL_SEQUENCE and EMB_COMP cannot be True'\n","  if DUAL_SEQUENCE:\n","    col_names=['nc_mod', 'nc_head']\n","  else: \n","    col_names=['compound']\n","\n","  train_dl = make_text_data_loader(train_data, col_names=col_names, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\n","  test_dl = make_text_data_loader(test_data, col_names=col_names, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\n","  val_dl = make_text_data_loader(val_data, col_names=col_names, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\n","\n","  return train_dl, test_dl, val_dl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D66cFe2h7ocg"},"source":["def determine_max_len(data):\n","  lengths = [len(tokenizer.encode(c)) for c in data['compound']]\n","  max_len_index = lengths.index(max(lengths))\n","  longest_input = data.iloc[max_len_index]['compound']\n","  tokens = tokenizer.encode(longest_input)\n","  return len(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJwVL68C0rf9"},"source":["class NounCompoundClassifier(torch.nn.Module):\n","  def __init__(self, num_classes, image_model=None, math_mode='add'):\n","    super(NounCompoundClassifier, self).__init__()\n","    self.bert = transformers.BertModel.from_pretrained(BERT_MODEL_NAME)\n","    if FREEZE_BERT:\n","      for param in self.bert.parameters():\n","        param.requires_grad = False\n","    self.drop = torch.nn.Dropout(p=0.3)\n","    linear_input_dims = self.bert.config.hidden_size if not MULTIMODAL else self.bert.config.hidden_size*2\n","    self.out = torch.nn.Linear(linear_input_dims, num_classes)\n","    self.softmax = torch.nn.Softmax(dim=1)\n","    self.multimodal = MULTIMODAL\n","\n","  def forward(self, input_ids, attention_mask, token_type_ids, img_vectors=None):\n","    _, pooled_output = self.bert(\n","        input_ids = input_ids, \n","        attention_mask = attention_mask, \n","        token_type_ids=token_type_ids\n","    )\n","    if self.multimodal:\n","      if MULTI_COMBO_MODE == 'concatenate':\n","        pooled_output = torch.cat((pooled_output, img_vectors), dim=1)\n","    output = self.drop(pooled_output.float())\n","    output = self.out(output)\n","    return self.softmax(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJUQV_H-E0kg"},"source":["def train_epoch(\n","    model, \n","    data_loader,\n","    image_data_loader, \n","    model_loss, \n","    optimizer,\n","    scheduler,\n","    num_examples\n","):\n","  model = model.train()\n","  losses = []\n","  correct_predictions = 0\n","\n","  if isinstance(image_data_loader, type(None)):\n","    for d in data_loader:\n","      input_ids = d['input_ids'].to(device)\n","      attention_mask = d['attention_mask'].to(device)\n","      token_type_ids = d['token_type_ids'].to(device)\n","      labels = d['labels'].to(device)\n","\n","      outputs = model( \n","          input_ids=input_ids,\n","          attention_mask=attention_mask,\n","          token_type_ids=token_type_ids\n","      )\n","      _, predictions = torch.max(outputs, dim=1)\n","      loss = model_loss(outputs, labels)\n","      correct_predictions += torch.sum(predictions == labels)\n","      losses.append(loss.item())\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","      optimizer.step()\n","      scheduler.step()\n","      optimizer.zero_grad()\n","  else:\n","    for d, img_d in zip(data_loader, image_data_loader):\n","      input_ids = d['input_ids'].to(device)\n","      attention_mask = d['attention_mask'].to(device)\n","      token_type_ids = d['token_type_ids'].to(device)\n","      labels = d['labels'].to(device)\n","      image_vectors = img_d['img_vector'].to(device)\n","\n","      outputs = model( \n","          input_ids=input_ids,\n","          attention_mask=attention_mask,\n","          token_type_ids=token_type_ids,\n","          img_vectors=image_vectors\n","        )\n","\n","      _, predictions = torch.max(outputs, dim=1)\n","      loss = model_loss(outputs, labels)\n","      correct_predictions += torch.sum(predictions == labels)\n","      losses.append(loss.item())\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","      optimizer.step()\n","      scheduler.step()\n","      optimizer.zero_grad()\n","  return correct_predictions.double() / num_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggA8Jg3tA3L6"},"source":["def train(model, scheduler, optimizer, model_loss, train_data, train_dl, train_img_dl, save_model=False):\n","  history = defaultdict(list)\n","  best_accuracy = 0\n","  for epoch in range(EPOCHS):\n","    print(f'Epoch {epoch+1} of {EPOCHS}')\n","    train_acc, train_loss = train_epoch(\n","      model=model,\n","      data_loader=train_dl, \n","      image_data_loader=train_img_dl,\n","      model_loss=model_loss,\n","      optimizer=optimizer,\n","      scheduler=scheduler,\n","      num_examples=len(train_data)\n","  )\n","  print(f'Train loss: {train_loss} -- train accuracy: {train_acc}')\n","  print('-'*20)\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","\n","  if save_model:\n","    model_name = f'{BERT_MODEL_NAME}_{SPLIT}_{GRAIN}_{EPOCHS}_{BATCH_SIZE}_{MAX_LEN}_{DUAL_SEQUENCE}_{EMB_COMP}'\n","    model_path = f'/content/drive/My Drive/{model_name}.bin' \n","    torch.save(model.state_dict(), model_path)\n","  return history\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuG1lugpHTIg"},"source":["def evaluate(model, data_loader, image_data_loader, model_loss, num_examples):\n","  model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  all_predictions = []\n","  all_labels = []\n","\n","  with torch.no_grad():\n","    if not MULTIMODAL: \n","      for d in data_loader:\n","        input_ids = d['input_ids'].to(device)\n","        attention_mask = d['attention_mask'].to(device)\n","        token_type_ids = d['token_type_ids'].to(device)\n","        labels = d['labels'].to(device)\n","\n","        outputs = model( \n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        _, predictions = torch.max(outputs, dim=1)\n","        all_predictions.append(predictions)\n","        all_labels.append(labels)\n","        loss = model_loss(outputs, labels)\n","        correct_predictions += torch.sum(predictions == labels)\n","        losses.append(loss.item())\n","    else:\n","      for d, img_d in zip(data_loader, image_data_loader):\n","        input_ids = d['input_ids'].to(device)\n","        attention_mask = d['attention_mask'].to(device)\n","        token_type_ids = d['token_type_ids'].to(device)\n","        labels = d['labels'].to(device)\n","        image_vectors = img_d['img_vector'].to(device)\n","\n","        outputs = model( \n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids, \n","            img_vectors=image_vectors\n","        )\n","        _, predictions = torch.max(outputs, dim=1)\n","        all_predictions.append(predictions)\n","        all_labels.append(labels)\n","        loss = model_loss(outputs, labels)\n","        correct_predictions += torch.sum(predictions == labels)\n","        losses.append(loss.item())\n","  return correct_predictions.double() / num_examples, np.mean(losses), all_predictions, all_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q7mq_Z_YK0FH"},"source":["def save_predictions(filename, data_list, predictions_list, labels_to_ids, save_preds_to):\n","  for i in range(len(predictions_list)):\n","    data = data_list[i]\n","    print(f'length of prediction series in iteration {i}: {len(predictions_list[i])}')\n","    preds = list(predictions_list[i])\n","    true_labels = list(data['nc_type'])\n","    predictions_dict = {'true_labels': true_labels, 'predicted_labels' : preds}\n","    print(list(predictions_list[i])[-5:])\n","    results_path =  f'/content/drive/My Drive/{save_preds_to}'\n","    if not os.path.exists(results_path):\n","      os.mkdir(results_path)\n","    results_final_path= f'{results_path}/{filename}'\n","    if not os.path.exists(results_final_path):\n","      os.mkdir(results_final_path)\n","    new_filename = f'{results_final_path}/{i}.json'\n","    with open(new_filename, 'w') as outfile:\n","      json.dump(predictions_dict, outfile)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7J-5VnrKStt"},"source":["def update_csv_with_results(history, val_acc, val_f1, val_loss, test_acc, test_f1, test_loss):\n","\n","  train_acc = history['train_acc'][-1].cpu().numpy() if device == 'cuda' else history['train_acc'][-1]\n","  train_loss = history['train_loss'][-1]\n","  val_acc = val_acc.cpu().numpy() if device == 'cuda' else val_acc\n","  test_acc = test_acc.cpu().numpy() if device == 'cuda' else test_acc\n","\n","\n","  new_results = {\n","      'model_name' : BERT_MODEL_NAME,\n","      'comp_embedded' : EMB_COMP,\n","      'dual_sequence' : DUAL_SEQUENCE,\n","      'split' : SPLIT,\n","      'grain' : GRAIN,\n","      'non_binary_removed' : REMOVE_NON_BINARY,\n","      'non_comp_removed' : REMOVE_NON_COMP,\n","      'train_acc_final' : train_acc,\n","      'train_loss_final' : train_loss,\n","      'val_acc' : val_acc,\n","      'val_loss' : val_loss,\n","      'val_f1_weighted' : val_f1,\n","      'test_acc' : test_acc,\n","      'test_loss' : test_loss,\n","      'test_f1_weighted' : test_f1,\n","      'epochs' : EPOCHS,\n","      'batch_size' : BATCH_SIZE,\n","      'max_len' : MAX_LEN,\n","      'padding' : 'to max len',\n","      'optimizer' : 'AdamW',\n","      'learning rate' : 2e-5,\n","      'correct_bias' : False,\n","      'loss_function' : 'CrossEntropy'\n","  }\n","\n","  csv_filename = '/content/drive/My Drive/Inga\\'s Thesis Folder/results_filtered/filtered_BERT_results_unimodal_auto.csv'\n","  if path.exists(csv_filename):\n","    results = pd.read_csv(csv_filename)\n","    results = results.append(new_results, ignore_index=True)\n","  else:\n","    results = pd.DataFrame(new_results, index=[0])\n","\n","  print(results)\n","  results.to_csv(csv_filename, index=False)\n","  print(\"SAVED RESULTS\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUxb2NtAKA2D"},"source":["def main(mode: str, iterations=1):\n","  if MULTIMODAL or UNIMODAL_COMPARABLE:\n","    image_model = get_image_model(IMAGE_MODEL)\n","    if 'apps' in image_model:\n","      print('ITS IN THERE!!')\n","    else:\n","      print('NOO SORRY')\n"," \n","  if mode == 'train':\n","    train_data, test_data, val_data, labels_to_ids, ordered_labels = prep_data()\n","    print('Length of train data:', len(train_data))\n","    print('Length of test data:', len(test_data))\n","    print('Length of val data:', len(val_data))\n","\n","    if JOIN_VAL_AND_TRAIN:\n","      train_data = pd.concat([train_data, val_data])\n","\n","    if MULTIMODAL or UNIMODAL_COMPARABLE:\n","      train_data = remove_non_existing(train_data, image_model, filter_on=['mod', 'head'])\n","      test_data = remove_non_existing(test_data, image_model, filter_on=['mod', 'head'])\n","      if not JOIN_VAL_AND_TRAIN:\n","        val_data = remove_non_existing(val_data, image_model, filter_on=['mod', 'head'])\n","      print('FILTERED DATA BY IMAGE MODEL')\n","      print(f'train length is now {len(train_data)}')\n","      print(f'test length is now {len(test_data)}')\n","      if not JOIN_VAL_AND_TRAIN:\n","        print(f'val length is now {len(val_data)}')\n","\n","    train_dl, test_dl, val_dl = prep_data_loaders(train_data, test_data, val_data)\n","\n","    if MULTIMODAL:\n","      train_img_data = prep_image_data(train_data, image_model)\n","      test_img_data = prep_image_data(test_data, image_model)\n","      \n","      if not JOIN_VAL_AND_TRAIN:\n","        val_img_data = prep_image_data(val_data, image_model)\n","\n","      train_img_dl = make_image_data_loader(train_img_data, BATCH_SIZE)\n","      test_img_dl = make_image_data_loader(test_img_data, BATCH_SIZE)\n","      \n","      if not JOIN_VAL_AND_TRAIN:\n","        val_img_dl = make_image_data_loader(val_img_data, BATCH_SIZE)\n","\n","    num_classes = len(labels_to_ids)\n","\n","    total_accuracy_test = 0\n","    total_f1_test = 0\n","    total_loss_test = 0\n","\n","    total_accuracy_val = 0\n","    total_f1_val = 0\n","    total_loss_val = 0\n","\n","    all_test_preds = []\n","    all_test_data = []\n","\n","    all_val_preds = []\n","    all_val_data = []\n","\n","    for i in range(iterations):\n","      \n","      model = NounCompoundClassifier(num_classes=num_classes)\n","      model.to(device)\n","\n","      if not FREEZE_BERT:\n","        optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","      elif FREEZE_BERT:\n","        optimizer = transformers.AdamW(filter(lambda p: p.requires_grad, model.out.parameters()), lr=0.1, correct_bias=False)\n","      num_steps = len(train_dl) * EPOCHS\n","      scheduler = transformers.get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps = 0,\n","        num_training_steps = num_steps\n","      )\n","      model_loss = torch.nn.CrossEntropyLoss().to(device)\n","      history = train(model, \n","                      scheduler=scheduler, \n","                      optimizer=optimizer, \n","                      model_loss=model_loss, \n","                      train_data=train_data, \n","                      train_dl=train_dl,\n","                      train_img_dl=train_img_dl if MULTIMODAL else None\n","                      )\n","\n","      ### TEST DATA ###\n","\n","      test_acc, test_loss, test_preds, test_labels = evaluate(\n","        model=model,\n","        data_loader=test_dl,\n","        image_data_loader=test_img_dl if MULTIMODAL else None,\n","        model_loss=model_loss,\n","        num_examples=len(test_data)\n","      )\n","\n","      print(GRAIN, SPLIT)\n","      print(f'test loss: {test_loss} -- test accuracy: {test_acc}\\n')\n","\n","      \n","\n","      test_preds_lst = [int(x) for ts in test_preds for x in ts]\n","      print('length of test preds:', len(test_preds_lst))\n","      test_labels_lst = [int(x) for ts in test_labels for x in ts]\n","      unique_label_ids = np.unique(train_data.label_id)\n","      print(unique_label_ids, '\\n', ordered_labels)\n","      #test_cls_report = classification_report(test_labels_lst, test_preds_lst, labels = unique_label_ids, target_names=ordered_labels, zero_division=0)\n","      test_f1 = f1_score(test_labels_lst, test_preds_lst, labels=unique_label_ids, average='weighted')\n","      #print('{0:*^80}'.format(' TEST CLASSFICATION REPORT '))\n","      #print(test_cls_report)\n","\n","      total_loss_test += test_loss\n","      total_accuracy_test += test_acc\n","      total_f1_test += test_f1\n","\n","      ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n","      test_preds_series = pd.Series(test_preds_lst).replace(ids_to_labels)\n","      print('length of test_preds_series: ', len(test_preds_series))\n","      all_test_preds.append(test_preds_series)\n","      all_test_data.append(test_data)\n","\n","\n","      ### VAL DATA ###\n","      if not JOIN_VAL_AND_TRAIN:\n","        val_acc, val_loss, val_preds, val_labels = evaluate(\n","          model=model,\n","          data_loader=val_dl,\n","          image_data_loader=val_img_dl if MULTIMODAL else None,\n","          model_loss=model_loss,\n","          num_examples=len(val_data)\n","        )\n","\n","        print(f'Val loss: {val_loss} -- val accuracy: {val_acc}\\n')\n","\n","        val_preds_lst = [int(x) for ts in val_preds for x in ts]\n","        val_labels_lst = [int(x) for ts in val_labels for x in ts]\n","        unique_preds_val = np.unique(np.array(val_preds_lst))\n","        unique_labels_val = np.unique(np.array(val_labels_lst))\n","        #val_cls_report = classification_report(val_labels_lst, val_preds_lst, labels=unique_label_ids, target_names=ordered_labels, zero_division=0)\n","        val_f1 = f1_score(val_labels_lst, val_preds_lst, labels=unique_label_ids, average='weighted')\n","        #print('{0:*^80}'.format(' VALIDATION CLASSFICATION REPORT '))\n","        #print(val_cls_report)\n","\n","        total_loss_val += val_loss\n","        total_accuracy_val += val_acc\n","        total_f1_val += val_f1\n","\n","        val_preds_series = pd.Series(val_preds_lst).replace(ids_to_labels)\n","        all_val_preds.append(val_preds_series)\n","        all_val_data.append(val_data)\n","      \n","      del model\n","\n","      ### SAVE GENERAL RESULTS TO A CSV FILE ###\n","      #update_csv_with_results(history, val_acc, val_f1, val_loss, test_acc, test_f1, test_loss)\n","\n","    if MULTIMODAL:\n","      mode_name = IMAGE_MODEL\n","    else:\n","      mode_name = 'unimodal'\n","    if FREEZE_BERT:\n","      freeze = 'frozen'\n","    else:\n","      freeze = 'not_frozen'\n","    test_preds_name = f'BERT_cb_predictions_test_{mode_name}_{freeze}_{BERT_MODEL_NAME}_{SPLIT}_{GRAIN}_{EPOCHS}_{BATCH_SIZE}_{MAX_LEN}_{DUAL_SEQUENCE}_{EMB_COMP}_{VEC_COMBO_MODE}_{MULTI_COMBO_MODE}'\n","    save_predictions(test_preds_name, all_test_data, all_test_preds, labels_to_ids, SAVE_PREDS_TO)\n","\n","    avg_accuracy_test = total_accuracy_test / iterations\n","    avg_loss_test = total_loss_test / iterations\n","    avg_f1_test = total_f1_test / iterations\n","\n","    if not JOIN_VAL_AND_TRAIN:\n","      val_preds_name = f'BERT_cb_predictions_val_{mode_name}_{freeze}_{BERT_MODEL_NAME}_{SPLIT}_{GRAIN}_{EPOCHS}_{BATCH_SIZE}_{MAX_LEN}_{DUAL_SEQUENCE}_{EMB_COMP}_{VEC_COMBO_MODE}_{MULTI_COMBO_MODE}'\n","      save_predictions(val_preds_name, all_val_data, all_val_preds, labels_to_ids, SAVE_PREDS_TO)\n","      \n","      avg_accuracy_val = total_accuracy_val / iterations\n","      avg_loss_val = total_loss_val / iterations\n","      avg_f1_val = total_f1_val / iterations\n","    \n","    print(f'AVERAGED RESULTS FOR {SPLIT} - {GRAIN} - {EPOCHS} epochs - freeze {FREEZE_BERT} - multimodal {MULTIMODAL} - {IMAGE_MODEL}')\n","    print(f'TEST RESULTS:')\n","    print(f'Average accuracy: {avg_accuracy_test} \\nAverage loss: {avg_loss_test} \\nAverage F1: {avg_f1_test} \\n')\n","    if not JOIN_VAL_AND_TRAIN:\n","      print(f'VAL RESULTS:')\n","      print(f'Average accuracy: {avg_accuracy_val} \\nAverage loss: {avg_loss_val} \\nAverage F1: {avg_f1_val} \\n')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zxHn_TkMaOX"},"source":["train_data, test_data, val_data, labels_to_ids, ordered_labels = prep_data()\n","all_data = pd.concat([train_data, test_data, val_data])\n","MAX_LEN = determine_max_len(all_data)\n","#MAX_LEN = 189\n","print(f'max len: {MAX_LEN}')\n","\n","main(mode='train', iterations=ITERATIONS)\n"],"execution_count":null,"outputs":[]}]}